{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_page_1(df):\n",
    "    '''\n",
    "    Cleans and extracts data from page 1 of the PDF.\n",
    "    \n",
    "    Inputs:\n",
    "        df: DataFrame with the raw data extracted from page 1 of the PDF)\n",
    "    Returns:\n",
    "        df1: cleaned DataFrame\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Defined the parameters to extract\n",
    "    parameters = [\n",
    "        \"Speed (m/s)\",\n",
    "        \"Cadence (steps/min)\",\n",
    "        \"Steps\",\n",
    "        \"Duration (s)\",\n",
    "        \"Distance (m)\"\n",
    "    ]\n",
    "    \n",
    "        #Lists for foot information, standard deviations and mean value\n",
    "    foot = ['Both'] * 5\n",
    "    \n",
    "    sd = ['NaN'] * 5\n",
    "    \n",
    "    values = []\n",
    "    \n",
    "    #Use regex to extract all numeric value and store in the list in a sequential order\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = str(row[1]).strip()\n",
    "    \n",
    "        nums = re.findall(r'\\d+\\.\\d+|\\d+', text)\n",
    "        if len(nums) > 0:\n",
    "            values.append(nums[0])\n",
    "    \n",
    "    \n",
    "    # print(f\"Total number of Extracted Values: {values}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Creates the new dataframe with 'parameters' and 'values' list\n",
    "    \n",
    "    df1 = pd.DataFrame(list(zip(parameters, foot, sd, values, )),\n",
    "                       columns=['Parameter', 'Foot', 'Standard Deviation', 'Mean' ])\n",
    "    \n",
    "    df1 = pd.melt(df1, id_vars=['Parameter', 'Foot'],\n",
    "                  value_vars=['Standard Deviation', 'Mean'],\n",
    "                  var_name='Statistic', value_name='Value')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\nPage 1 cleaning is complete\")\n",
    "    # print(df1)\n",
    "    return df1\n",
    "\n",
    "\n",
    "def clean_page_3(df):\n",
    "    \n",
    "    '''\n",
    "    Cleans and extracts data from page 1 of the PDF.\n",
    "    \n",
    "    Inputs:\n",
    "        df: DataFrame with the raw data extracted from page 3 of the PDF)\n",
    "    Returns:\n",
    "        df3: cleaned DataFrame of page 3\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Define paramers to extract and foot information\n",
    "    parameters_3 = [\n",
    "        \"Step Length (cm)\",\n",
    "        \"Step Length (cm)\",\n",
    "        \"Step Width (cm)\",\n",
    "        \"Step Width (cm)\",\n",
    "        \"Stride Length (cm)\",\n",
    "        \"Stride Length (cm)\"\n",
    "        ]\n",
    "    \n",
    "    foot_3 = [\n",
    "    \"Left\",\n",
    "    \"Right\",\n",
    "    \"Left\",\n",
    "    \"Right\",\n",
    "    \"Left\",\n",
    "    \"Right\"\n",
    "    ]\n",
    "    \n",
    "    # Empty lists to store extracted value from the PDF\n",
    "    sd_3 = []\n",
    "    \n",
    "    value_3 = []\n",
    "    \n",
    "    parameter_number = 3\n",
    "    \n",
    "    j = 1 #Column index\n",
    "    \n",
    "# Split data at the '+/-' sign to extract mean value and standard deviation separately\n",
    "    for i in range(parameter_number):\n",
    "        print(f'in this iteration, j ={j} and i ={i}')\n",
    "        left_value, left_sd = df.iloc[2, j].split(' ± ')\n",
    "        right_value,right_sd = df.iloc[3, j].split(' ± ')\n",
    "                    \n",
    "# Append data into the empty lists created earlier\n",
    "        value_3.append(left_value)\n",
    "        sd_3.append(left_sd)\n",
    "        value_3.append(right_value)\n",
    "        sd_3.append(right_sd)\n",
    "        \n",
    "        j+=2\n",
    "        \n",
    "# Generating a new DataFrame using extracted data\n",
    "    df3 = pd.DataFrame(list(zip(parameters_3, foot_3, sd_3, value_3, )),\n",
    "                       columns=['Parameter', 'Foot', 'Standard Deviation', 'Mean' ])\n",
    "    \n",
    "    df3 = pd.melt(df3, id_vars=['Parameter', 'Foot'],\n",
    "                value_vars=['Standard Deviation', 'Mean'],\n",
    "                var_name='Statistic', value_name='Value')\n",
    "    \n",
    "    print(\"\\nPage 3 cleaning is complete\")\n",
    "    return df3\n",
    "    \n",
    "    \n",
    "def clean_page_4(df):\n",
    "    \n",
    "    '''\n",
    "    Cleans and extracts data from page 4 of the PDF.\n",
    "    \n",
    "    Inputs:\n",
    "        df: DataFrame with the raw data extracted from page 4 of the PDF)\n",
    "    Returns:\n",
    "        df3: cleaned DataFrame of page 4\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "# Define parameters and foot information\n",
    "    parameters_4 = [\n",
    "    \"Gait Cycle (s)\",\n",
    "    \"Gait Cycle (s)\",\n",
    "    \"Swing Phase (s)\",\n",
    "    \"Swing Phase (s)\",\n",
    "    \"Stance Phase (s)\",\n",
    "    \"Stance Phase (s)\"\n",
    "    ]\n",
    "    foot_4 = [\n",
    "    \"Left\",\n",
    "    \"Right\",\n",
    "    \"Left\",\n",
    "    \"Right\",\n",
    "    \"Left\",\n",
    "    \"Right\"\n",
    "    ]\n",
    "    \n",
    "    value_4 = []\n",
    "    \n",
    "    sd_4 = []\n",
    "    \n",
    "    j = 1 # Counter to iterates through each column\n",
    "    k = 2 # Row index for extracting the left foot data\n",
    "    l = 3 # Row index for extracting right foot data\n",
    "\n",
    "    for i in range(3):\n",
    "        \n",
    "        \n",
    "        print(f'in this iteration, j ={j} and i ={i}')\n",
    "        \n",
    "        R0C0 = df.iloc[0, 0]\n",
    "        R2C1 = df.iloc[2, 1]\n",
    "        \n",
    "## Handle the four different data loyut possibilities\n",
    "\n",
    "\n",
    "# Possibility 1\n",
    "        if (R0C0 == '') and (R2C1 ==''):\n",
    "            # To extract Gait Cycle\n",
    "            if i == 0:\n",
    "                print('left : ' + df.iloc[k-1, j+1] )\n",
    "                print('right : ' + df.iloc[l-1, j+1] )\n",
    "                \n",
    "                left_value, left_sd = df.iloc[k-1, j+1].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[l-1, j+1].split(' ± ')   \n",
    "            # To extract Swing phase\n",
    "            if i == 1:\n",
    "                left_value, left_sd = df.iloc[k+4, j].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[l+4, j].split(' ± ')      \n",
    "            #To extract stance phase\n",
    "            if i == 2:         \n",
    "                left_value, left_sd = df.iloc[6, 4].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[7, 4].split(' ± ')                     \n",
    "# Possbility 2        \n",
    "        elif R2C1 == '':\n",
    "            if i == 0:\n",
    "                print('left : ' + df.iloc[k, j+1])\n",
    "                print('right : ' + df.iloc[l, j+1])\n",
    "                      \n",
    "                left_value, left_sd = df.iloc[k, j+1].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[l, j+1].split(' ± ')     \n",
    "            if i == 1:\n",
    "                left_value, left_sd = df.iloc[k+5, j].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[l+5, j].split(' ± ')  \n",
    "            if i == 2:        \n",
    "                left_value, left_sd = df.iloc[7, 4].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[8, 4].split(' ± ')          \n",
    "# Possibility 3          \n",
    "        elif R0C0 == '':\n",
    "            if i == 0:\n",
    "                print('left : ' + df.iloc[k-1, j])\n",
    "                print('right : ' + df.iloc[l-1, j])\n",
    "                      \n",
    "                left_value, left_sd = df.iloc[k-1, j].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[l-1, j].split(' ± ')   \n",
    "            if i == 1:\n",
    "                left_value, left_sd = df.iloc[k+4, j].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[l+4, j].split(' ± ')  \n",
    "                 \n",
    "            if i == 2:        \n",
    "                left_value, left_sd = df.iloc[6, 4].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[7, 4].split(' ± ')          \n",
    "# Possibility 4            \n",
    "        else:\n",
    "            if i == 0:\n",
    "                print('left : ' + df.iloc[k , j])\n",
    "                print('right : ' + df.iloc[l, j])\n",
    "                      \n",
    "                      \n",
    "                left_value, left_sd = df.iloc[k , j].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[l, j].split(' ± ')\n",
    "            if i == 1:\n",
    "                left_value, left_sd = df.iloc[k + 5 , j].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[l + 5, j].split(' ± ')        \n",
    "            if i == 2:        \n",
    "                left_value, left_sd = df.iloc[7, 4].split(' ± ')\n",
    "                right_value,right_sd = df.iloc[8, 4].split(' ± ')  \n",
    "                \n",
    "# Appending the extracted values to the lists created                \n",
    "        value_4.append(left_value)\n",
    "        sd_4.append(left_sd)\n",
    "        value_4.append(right_value)\n",
    "        sd_4.append(right_sd)\n",
    "        \n",
    "    # Creating a clean DataFrame with the extracted data\n",
    "        \n",
    "    df4 = pd.DataFrame(list(zip(parameters_4, foot_4, sd_4, value_4, )),\n",
    "                    columns=['Parameter', 'Foot', 'Standard Deviation', 'Mean' ])\n",
    "        \n",
    "    df4 = pd.melt(df4, id_vars=['Parameter', 'Foot'],\n",
    "                value_vars=['Standard Deviation', 'Mean'],\n",
    "                var_name='Statistic', value_name='Value')\n",
    "    \n",
    "    print(\"\\nPage 4 cleaning is complete\")\n",
    "    \n",
    "    return df4\n",
    "    \n",
    "def clean_page_18(df):\n",
    "    print(df)\n",
    "\n",
    "    '''\n",
    "        Cleans and extracts data from page 18 of the PDF.\n",
    "        \n",
    "        Inputs:\n",
    "            df: DataFrame with the raw data extracted from page 18 of the PDF)\n",
    "        Returns:\n",
    "            df18: cleaned DataFrame of page 18\n",
    "        \n",
    "    '''\n",
    "    \n",
    "# Predifined list of the parameters and foot infomration of the final DataFrame\n",
    "    parameters_18 = [\n",
    "    \"Ankle IC Angle (deg)\",\n",
    "    \"Ankle IC Angle (deg)\",\n",
    "    \"Ankle TC Angle (deg)\",\n",
    "    \"Ankle TC Angle (deg)\"\n",
    "    ]\n",
    "    \n",
    "    foot_18 = [\n",
    "    \"Left\",\n",
    "    \"Right\",\n",
    "    \"Left\",\n",
    "    \"Right\"\n",
    "    ]\n",
    " \n",
    "    \n",
    "    value_18 = []\n",
    "    \n",
    "    sd_18 = []\n",
    "\n",
    "\n",
    "\n",
    "    left_value, left_sd = df.iloc[4, 7].split(' ± ')\n",
    "    right_value,right_sd = df.iloc[5, 7].split(' ± ')\n",
    "\n",
    "                \n",
    "# Append data into the empty lists created earlier\n",
    "    value_18.append(left_value)\n",
    "    sd_18.append(left_sd)\n",
    "    value_18.append(right_value)\n",
    "    sd_18.append(right_sd)\n",
    "    \n",
    "    left_value, left_sd = df.iloc[4, 8].split(' ± ')\n",
    "    right_value,right_sd = df.iloc[5, 8].split(' ± ')\n",
    "    \n",
    "    value_18.append(left_value)\n",
    "    sd_18.append(left_sd)\n",
    "    value_18.append(right_value)\n",
    "    sd_18.append(right_sd)\n",
    "\n",
    "# Create a new DataFrame with the extracted data\n",
    "    df18 = pd.DataFrame(list(zip(parameters_18, foot_18, sd_18, value_18, )),\n",
    "                       columns=['Parameter', 'Foot', 'Standard Deviation', 'Mean' ])\n",
    "        \n",
    "    df18 = pd.melt(df18, id_vars=['Parameter', 'Foot'],\n",
    "                value_vars=['Standard Deviation', 'Mean'],\n",
    "                var_name='Statistic', value_name='Value')\n",
    "    \n",
    "    print(\"\\nPage 3 cleaning is complete\")\n",
    "    # print(df18)\n",
    "\n",
    "    return df18\n",
    "        \n",
    "        \n",
    "     \n",
    "\n",
    "            \n",
    "# Mapping the page numbers to their respective cleaning functions\n",
    "page_cleaning_switch = {\n",
    "    1: clean_page_1,\n",
    "    3: clean_page_3,\n",
    "    4: clean_page_4,\n",
    "    18: clean_page_18\n",
    "}\n",
    "\n",
    "\n",
    "def extract_data_from_pdfs(pdf_dir, pages, output_dir):\n",
    "    '''\n",
    "    Extracts and cleans data from specified pages and combines all into a single CSV\n",
    "    \n",
    "    Parameters:\n",
    "        pdf_dir(str) : Directory containing PDF files.\n",
    "        pages (list): Pages to extract, currentlym only suppurs pages 1,3,4 and 18\n",
    "        output_dir (str): Directory to save the output CSV file\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find all PDF files in the directory\n",
    "    pdf_files = glob.glob(os.path.join(pdf_dir, '*.pdf'))\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found in the specified directory.\")\n",
    "        return\n",
    "    \n",
    "    df_per_trajec = []\n",
    "    \n",
    "    \n",
    "    # List to store all trajectory DataFrames\n",
    "    combined_trajectory_dfs = []\n",
    "\n",
    "    # Process each PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        Trajec_name = os.path.basename(pdf_file).split('.')[0].split('-')[0]\n",
    "        dfs_to_combine = []\n",
    "        \n",
    "        \n",
    "        print(f\"\\nProcessing file: {pdf_file}\")\n",
    "        \n",
    "\n",
    "        # Extract tables from specified pages\n",
    "        tables = camelot.read_pdf(\n",
    "            pdf_file,\n",
    "            pages=pages,\n",
    "            flavor='stream',  # Use 'stream' or 'lattice' depending on your PDFs\n",
    "            strip_text='\\n',  # Remove line breaks within cells\n",
    "            edge_tol=500,     # Tolerance for table edge detection\n",
    "            row_tol=20,       # Tolerance for row detection\n",
    "        )\n",
    "\n",
    "        if tables.n == 0:\n",
    "            print(f\"    No tables found in {pdf_file} on pages {pages}.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Found {tables.n} tables in {pdf_file} on pages {pages}.\")\n",
    "\n",
    "        # Iterate over each extracted table\n",
    "        for i, table in enumerate(tables, start=1):\n",
    "            \n",
    "            df = table.df  # Get the table as a DataFrame\n",
    "            df = df.dropna(how='all')           # Drop empty rows\n",
    "            df = df.dropna(axis=1, how='all')   # Drop empty columns\n",
    "\n",
    "            # Get the page number from the table\n",
    "            page_number = table.page\n",
    "            \n",
    "            # Select the appropriate cleaning function based on page number\n",
    "            clean_function = page_cleaning_switch.get(page_number)\n",
    "            if clean_function:\n",
    "                cleaned_df = clean_function(df)\n",
    "                dfs_to_combine.append(cleaned_df)\n",
    "            else:\n",
    "                print(f\"No cleaning function defined for page {page_number}\")\n",
    "                continue\n",
    "        if dfs_to_combine:    \n",
    "            df_per_trajec = pd.concat(dfs_to_combine, ignore_index=True)\n",
    "            \n",
    "            df_per_trajec = df_per_trajec.rename(columns={'Value': f'{Trajec_name}'})\n",
    "            print(df_per_trajec)\n",
    "            \n",
    "            combined_trajectory_dfs.append(df_per_trajec)   \n",
    "            \n",
    "            # Combine all cleaned DataFrames vertically into one DataFrame for the current PDF\n",
    "\n",
    "    if combined_trajectory_dfs:\n",
    "        common_columns = combined_trajectory_dfs[0].iloc[:, :3]\n",
    "        unique_columns = pd.concat([df.iloc[:, 3] for df in combined_trajectory_dfs], axis=1)\n",
    "        final_df = pd.concat([common_columns, unique_columns], axis=1)\n",
    "        final_df = final_df.T\n",
    "    # print(final_df)\n",
    "    \n",
    "            \n",
    "            # df_per_trajec = pd.concat([common_columns, unique_columns], axis=1 )\n",
    "    \n",
    "        \n",
    "            # # Rename the Values columns to include the trajectory name\n",
    "            # df_per_trajec = df_per_trajec.rename(columns={\n",
    "            #     'Value': f'{Trajec_name}'\n",
    "            # })\n",
    "            \n",
    "            \n",
    "            # # Append the prepared DataFrame to the list\n",
    "            # combined_trajectory_dfs.append(df_per_trajec)\n",
    "            # # print(combined_trajectory_dfs)\n",
    "       \n",
    "            \n",
    "    # # Combine all trajectories horizontally\n",
    "    # final_df = pd.concat(combined_trajectory_dfs, axis=1)\n",
    "\n",
    "    #     # # Reset index to turn 'Parameter' and 'Foot' back into columns\n",
    "    #     # final_df = final_df.reset_index()\n",
    "    # # print(final_df)\n",
    "        \n",
    "    final_df.to_csv(output_dir)\n",
    "            \n",
    "            \n",
    "\n",
    "                \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "            \n",
    "                \n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "# -----------------------------\n",
    "# 3. Example Usage\n",
    "# -----------------------------\n",
    "\n",
    "pdf_dir = r\"C:\\Users\\yangk\\OneDrive - National University of Singapore\\P013 SmartSole\\60. Verification testing\\TEST-003B\\Results\\MotionCloud Reports\"\n",
    "pages_to_extract = '1,3,4,18'\n",
    "output_dir = r'C:\\Users\\yangk\\OneDrive\\Documents\\HATCH\\Programming Learning\\Gait Report PDF Generator\\Extracted Data from MotionCloud Report\\extracted_data.csv'\n",
    "\n",
    "# Call the extraction function\n",
    "extracted_data = extract_data_from_pdfs(pdf_dir, pages_to_extract, output_dir)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
